# -*- coding: utf-8 -*-
"""Copy of CMSC320 Etymology Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZpX1F7ZlZIK1s7JuSJbijwjtaFVAt6H5
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
#rtatman_english_word_frequency_path = kagglehub.dataset_download('rtatman/english-word-frequency')
#bilalelebi_dataset_path = kagglehub.dataset_download('bilalelebi/dataset')

print('Data source import complete.')

"""# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from matplotlib import pyplot as plt

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from matplotlib import pyplot as plt


etymology = pd.read_csv('etymology.csv')
display(etymology)

etymology.columns

df = etymology[etymology['lang'] == 'English']
engl = df.drop(['lang', 'term_id', 'related_term_id', 'position', 'group_tag', 'parent_tag', 'parent_position'], axis = 1)
display(engl)

engl['related_lang'].value_counts().median()
engl['reltype'].unique()

# (engl['related_lang'] != 'English')
related_counts = engl[(engl['reltype'] == 'borrowed_from')]['related_lang'].value_counts()
piechart_lst = related_counts[related_counts >= 150]
other = pd.Series({'Other': engl[(engl['reltype'] == 'borrowed_from')]['related_lang'].value_counts()[engl['related_lang'].value_counts() < 150].sum()})
piechart_series = pd.concat([piechart_lst, other])
piechart_series

piechart_series.plot.pie(figsize = (10, 10), title = 'Number of English Words Borrowed from Each Language', fontsize = 8)

engl[engl['term'] == 'the']

freqs = pd.read_csv('unigram_freq.csv')
freqs.set_index('word', inplace = True)
freqs

# engl['frequency'] = engl['term'].apply(lambda word: freqs.loc[word] if (word in freqs.index) else -1)
df = engl.set_index('term').join(freqs).sort_values('count', ascending = False).dropna(subset = ['count'])
df['freq'] = df['count'] / freqs['count'].sum()
df

from scipy.stats import chi2_contingency

# load csv
df = pd.read_csv("etymology.csv")
reltype_counts = df['reltype'].value_counts()
reltype_counts

# hypothesis test
observed = reltype_counts.values
n = sum(observed)
k = len(observed)
expected = np.array([n/k] * k)
chi2, p_value = chi2_contingency([observed, expected])[0:2]

print(f"Chi-square statistic: {chi2}")
print(f"p-value: {p_value}")

# bar
other = 0
labels = []
values = []

for reltype in reltype_counts.index:
  if reltype_counts[reltype] < 2000:
    other += reltype_counts[reltype]
  else:
    labels.append(reltype)
    values.append(reltype_counts[reltype])

plt.bar([*labels, "other"], [*values, other])
plt.xticks(rotation=80)
plt.title("Distribution of relationship types")
plt.xlabel("Relationship type")
plt.ylabel("Occurences")